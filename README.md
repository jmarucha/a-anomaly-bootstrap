# S-matrix numerical bootstrap

![cluster goes brrr](./doc/brrr.png)

## Quick start guide

1. Upload the package to the cluster. SDPB problems tend to occupy lot of space, so using `/scratch` is recommended.
2. Set the directory of spherical integrals to `/work/fsl/branes_int` or compute your own grid (don't).
3. Fill config with the data points of your dreams. Ensure that `dry_run = false` and `use_debug_partition = false` are set.
4. Prepare SDPB problems using `construct_problems.py`
5. Run SDPB problems using `run_problems.py`
6. *haha, cluster goes brrrrr*
7. Have the problem of cosmological constant solved.

## How to cluster

There are three clusters available, `deneb`, `fidis` and `helvetios` (`helvetios` being the most recent). To log in to a frond-end use SSH over EPFL's VPN. Adresses  are:

`deneb1.epfl.ch`

`deneb2.epfl.ch`

`fidis.epfl.ch`

`helvetios.epfl.ch`

For Windows PuTTY is the best option (and Filezilla for uploading files, with `sftp://fidis.epfl.ch` as host).

The magic comand for scheduling tasks on the cluster is
`sbatch your_bash_script.run`. There are some nice examples at [scitas-examples](https://c4science.ch/diffusion/SCEXAMPLES/scitas-examples.git) git repo. The finished tasks will produce some slurp files, which are runtime info and stdout of the process.

You can use `sbatch --partition=debug your_bash_script.run` for short, testing-purpose jobs, however the computing time is super-limited and it is limited to one job at the time. This can be enabled in config as well, using `use_debug_partition = true`.

## List of programs

### `mathematica/amplitudes.m`

This is what prepares the linear problem symbolically, so construct matrices `M(s)` to be used in SPDB.

### `mathematica/bootstrap.m`

This script converts abstract s-dependent problem of semi-definitness to set of problems on lattice of discrete values of s. The key functions are:

`constructSDPProblem[valN][valMaxN, valMaxSpin]`

`constructSDPData[valN][valMaxN, valMaxSpin]`

with parametres

- `valN` - O(N) (global) symmetry of the problem
- `valMaxN` - the maximal power of s (and t and u) to which the amplitude function is expanded. Don't be surprised by its lack in the code - it works via defining a global to be used in `amplitudes.m`
- `valMaxSpin` - biggest Lagendre polynomial into which amplitude is expanded (or into how many partial waves it is decomposed)

The `constructSDPData` is a helper function used to evaluate the gridpoints, `constructSDPProblem` uses `amplitudes.m` to construct a problem, set up goals and invokes `constructSDPData` to evaluate whole grid.

Files from `sample_points/data` define the grid and provide some useful integrals for partial wave decomposition and the output will be a gargantous set of Mathematica files defining SDPB problems.

### `mathematica/util.m`

These are some helper functions for matrix manipulation.

### `sample_points/evaluation.m`

Populate the folder `sample_points/data` with values of integrals from `sample_points/list_objects.m` at precision given at `sample_points/util`. These are partial wave decompositions of basis of amplitudes given in ansatz in `amplitudes.m`.

### `sample_points/generate_objects.m`

Populates the file `sample_points/list_objects.m` up to given order in ρ's. The definitions come from `amplitudes.m`.

### `sample_points/list_objects.m`

Contains the integrals to be precomputed in `sample_points/evaluation.m` with the partial wave decomposition of powers of ρ at given parameter s.

### `sample_points/util.m`

Some commons used by all scripts in the folder. The important ones are `s0` which is point around which ρ is expanded, `mass` which is 0 for our phonons and `working_precision` which is the favorite large number. The favorite large number of Dennis is `150`, which is proved to be a best number. `numberGridPoints` is number of different `s` for which unitarity will be tested.

### `sample_points/run_samples.py`

Schedules the execution of `sample_points/evaluation.m` on the cluster.

### `sdpb_binaries/sdp2input`

This binary converts Mathematica files generated by previous stuff to SDPB input

### `sdpb_binaries/sdpb`

Unholy power of S-Matrix bootstrap trapped into a single binary.

## Troubleshooting

### 1. No problems are generated by `generate_problems`

This may be related to lack of proper permissions. `chmod +x sdpb_binaries/sdp2input` should fix it.

### 2. SDPB dies immidiatly after starting

Well, shit happens. If you, dummy, tried to generate problems in your `/home` dir, you are probably out of space.

### 3. SDPB doesn't generate out.txt

Either you run out of space, or the process timed out. Luckily SDPB saves checkpoints every 15 minutes, so just rerun `run_problems.py` and it will continue for another 12 hours.

### 4. There is some crap about Wolfram authentication file missing

This is harmless, but if it really hurts your eyes, run from the command line

```bash
module load mathematica
wolframscript --cloud -c 'Print["FSL is awesome"]'
```

Log in to your Wolfram account and it'll be gone forever.

### 5. I cannot find slurm files

Well, that's a bit shame of mine I didn't put them in one place. The ones generated by `construct_problems.py` are in `problems` and `run_problems.py` are in `sdpb_binaries`.

### 6. Is there any way to clear all logs, temporary bash scripts and problem files?

No :<

## Plans for the future

The intention of separating `amplitudes.m` and `bootstrap.m` is to keep the latter specific problem agnostic, however we massively failed at separating these two logics.

Also scripts for generating the grid are in desperate need for refactoring, the separate folders for logs would be useful, and saving checkpoints somewhere else than in `problems` folder would be handy.
